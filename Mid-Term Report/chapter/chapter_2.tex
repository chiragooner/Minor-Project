    \chapter{Literature Review}
    The introduction of Artificial Intelligence has greatly enhanced the interaction capabilities of people with hearing and speech related disabilities and impairments. With there being millions of people suffering from these disabilities, the use of suitable lip reading applications and models can allow them to engage in conversations, thus making them be connected to the real world.However, developing such a model is challenging for both designers and researchers. These models should be well designed,
perfected, and integrated into smart devices to be widely available to all people in need of speech understanding assistance.
\\Lip reading can be conducted on the letter,word,sentence,digit or phrase level. It can also be based on video, voice, video with voice or video without voice as input. There have been studies focused on speaker-independent lip reading by adapting a system using the Speaker Adaptive Training (SAT), which was initially used in the speech recognition field.\cite{almajai2016improved}. 
Research has also been done towards developing an audio-visual speech enhancement framework that operates at two levels: a novel deep-learning based lip-reading regression model and an enhanced, visually-derived Wiener filter for estimating the clean audio power spectrum.\cite{adeel2019lip}
The paper\cite{miled2023lip} uses CNN and Bi-GRU (Bi directional Gated recurrent unit). According to this algorithm, the system is decomposed into two blocks. The first block consist of lip segmentation. The mouth region is extracted using Haar Cascade classifier. Then hybrid active contours model with an improved of the edge by a designed filter is proposed. The second block consists to classify word lip-reading. First, deep convolutional neural network (CNN) is applied to extract frame features from videos who take the results of first block as inputs. Second, the Bi-GRU with two hidden layers is followed by a global average pooling layer. Finally, the word classification results are obtained by Softmax layer. Using segmented lip inputs can yield stronger features, and vastly improve recognition performance.
The paper\cite{assael2016lipnet} proposes a novel lip-reading driven deep learning approach for speech enhancement that leverages the strengths of deep learning and analytical acoustic modeling. The proposed audio-visual speech enhancement framework operates at two levels: a novel deep learning based lip-reading regression model and an enhanced, visually-derived Wiener filter for estimating the clean audio power spectrum. This  discusses the challenges of lipreading and presents LipNet, a model that can map a sequence of video frames to text, trained entirely end-to-end. On the GRID corpus dataset, LipNet achieves 95.2\% accuracy in sentence-level, overlapped speaker split tasks.
\\The\cite{gutierrez2017lip} uses the  MIRAVL-VC1 dataset which outperforms previous datasets in various aspects. It uses modified form of residual network architecture and uses various techniques in data processing, augmentation and visualization to overcome the scarcity of data and improve the performance. Possible insight into possible improvements and future work in expanding the scale and generalization of the model.The paper\cite{el2023developing} attempts to use phonemes as a classification schema for lip‚Äêreading sentences to explore an alternative schema and to enhance system performance. In the paper\cite{li2023improving}, they try to improve the accuracy of speech recognition in noisy environments by improving the lip reading performance and the cross-modal fusion effect.The experimental results show that their method could achieve a significant improvement over speech recognition models in different noise environments.The paper\cite{zhang2023research} makes GhostNet better by creating Efficient-GhostNet. It improves performance with fewer parameters using a new method for communication within the network, making it more efficient.The improved Efficient-GhostNet is used to perform lip spatial feature extraction, and then the extracted features are inputted to the GRU network to obtain the temporal features of the lip sequences, and finally for prediction.The article\cite{hao2020survey} looks closely at different deep learning methods for lipreading, discussing how they are structured. It also lists various lipreading databases, providing details about them and the techniques used. The paper ends by talking about the challenges in current lipreading methods and suggesting possible future research directions.Lastly we studied about usage of extraction of audio as well as visual features from a video for predicting the spoken sentence/word in\cite{adeel2019lip}.The usage of both the features definitely increases the accuracy of the result but that also increases the complexity of the project as models to extract both audio and visual features are to be trained and the database to be used requires both audio and video essence which is more complex to collect and process than the database containing only visual features.

      